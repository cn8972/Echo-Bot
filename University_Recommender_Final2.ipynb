{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZryvfar8yBMOJ/GZCNynD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cn8972/Echo-Bot/blob/main/University_Recommender_Final2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 699
        },
        "id": "Ps7BrTVaaDKF",
        "outputId": "7f889d05-b67b-4417-94a7-0b7d069ab217"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://981e0287e54ac73a8c.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://981e0287e54ac73a8c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================ Gradio URL ================\n",
            " Public: https://981e0287e54ac73a8c.gradio.live\n",
            "  Local:  N/A\n",
            "============================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ================== University Recommender — Colab + KaggleHub + Gradio (Single Cell) ==================\n",
        "# 1) Installs kagglehub and gradio\n",
        "# 2) Auto-downloads and loads Kaggle dataset: nitishabharathi/university-recommendation\n",
        "# 3) Builds an interactive Gradio UI with a public share URL\n",
        "# 4) Recognizes `univName` / `univname` / `univ_name` for university names\n",
        "# =======================================================================================================\n",
        "\n",
        "# -------- Install --------\n",
        "import sys, subprocess\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"gradio>=4.0.0\", \"pandas\", \"numpy\", \"kagglehub[pandas-datasets]\"])\n",
        "\n",
        "# -------- Imports --------\n",
        "import os\n",
        "import io\n",
        "import glob\n",
        "import typing as T\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "\n",
        "# KaggleHub\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "# ---------------------\n",
        "# Column aliases (includes `univName`)\n",
        "# ---------------------\n",
        "COLUMN_MAP = {\n",
        "    \"university_name\": [\n",
        "        \"University\", \"university\", \"University Name\", \"UNIVERSITY NAME\",\n",
        "        \"College\", \"college\", \"College Name\", \"college_name\",\n",
        "        \"Institution\", \"institution\", \"Institution Name\", \"inst_name\",\n",
        "        \"University_Name\", \"university_name\", \"univ\", \"u_name\", \"name\",\n",
        "        \"univName\", \"univname\", \"univ_name\"\n",
        "    ],\n",
        "    \"university_id\": [\"university_id\", \"uni_id\", \"college_id\", \"univ_id\"],\n",
        "    \"rank\": [\"Rank\", \"ranking\", \"University Rating\", \"University_Rating\", \"Rank*\", \"rank\", \"rank_percentile\"],\n",
        "    \"region\": [\"Region\", \"State\", \"Location\", \"Country\", \"region\", \"state\", \"location\", \"country\"],\n",
        "    \"gre\": [\"GRE\", \"GRE Score\", \"GreScore\", \"GRE_Score\", \"GRE_New\", \"gre\", \"gre_score\", \"gre_new\"],\n",
        "    \"toefl\": [\"TOEFL\", \"TOEFL Score\", \"TOEFL_Score\", \"toefl\", \"toefl_score\", \"ielts\"],\n",
        "    \"gpa\": [\"GPA\", \"CGPA\", \"cgpa\", \"GPA_Score\", \"gpa\", \"c_gpa\", \"undergrad_gpa\"],\n",
        "    \"sop\": [\"SOP\", \"Statement of Purpose\", \"SOP_Score\", \"sop\", \"sop_score\"],\n",
        "    \"lor\": [\"LOR\", \"LOR \", \"Letter of Recommendation\", \"LOR_Score\", \"lor\", \"lor_score\"],\n",
        "    \"research\": [\"Research\", \"Research_Flag\", \"research\", \"research_flag\"],\n",
        "    \"label\": [\"admit\", \"admission\", \"admission_decision\", \"decision\", \"accepted\", \"target\", \"label\"]\n",
        "}\n",
        "\n",
        "WEIGHTS = {\"rank\": -1.0, \"gre\": 0.40, \"toefl\": 0.20, \"gpa\": 0.40, \"sop\": 0.10, \"lor\": 0.10, \"research\": 0.15}\n",
        "DEFAULT_BANDS = {\"reach\": 0.40, \"target\": 0.70}\n",
        "TOP_N_DEFAULT = 5\n",
        "\n",
        "# ---------------------\n",
        "# Utilities\n",
        "# ---------------------\n",
        "def _lc(s: str) -> str:\n",
        "    return s.strip().lower().replace(\"\\u00a0\", \" \")\n",
        "\n",
        "def build_column_index(df: pd.DataFrame) -> dict:\n",
        "    return {_lc(c): c for c in df.columns}\n",
        "\n",
        "def find_col_ci(df: pd.DataFrame, candidates: T.List[str]) -> T.Optional[str]:\n",
        "    idx = build_column_index(df)\n",
        "    for cand in candidates:\n",
        "        key = _lc(cand)\n",
        "        if key in idx:\n",
        "            return idx[key]\n",
        "    return None\n",
        "\n",
        "def first_existing(df: pd.DataFrame, aliases: T.List[str]) -> T.Optional[str]:\n",
        "    return find_col_ci(df, aliases)\n",
        "\n",
        "def minmax(s: pd.Series) -> pd.Series:\n",
        "    s = pd.to_numeric(s, errors=\"coerce\")\n",
        "    if s.notna().sum() == 0 or s.max() == s.min():\n",
        "        return pd.Series(np.zeros(len(s)), index=s.index)\n",
        "    return (s - s.min()) / (s.max() - s.min())\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "def normalize_user_prior(key: str, val: float) -> float:\n",
        "    if val is None:\n",
        "        return 0.0\n",
        "    v = float(val)\n",
        "    ranges = {\"gre\": (260.0, 340.0), \"toefl\": (0.0, 120.0), \"gpa\": (0.0, 4.0), \"sop\": (1.0, 5.0), \"lor\": (1.0, 5.0)}\n",
        "    if key in ranges:\n",
        "        lo, hi = ranges[key]\n",
        "        if hi <= lo:\n",
        "            return 0.5\n",
        "        return float(np.clip((v - lo) / (hi - lo), 0.0, 1.0))\n",
        "    if key == \"research\":\n",
        "        return 1.0 if v > 0 else 0.0\n",
        "    return 0.5\n",
        "\n",
        "def validate_dataset(df: pd.DataFrame) -> None:\n",
        "    has_name = first_existing(df, COLUMN_MAP[\"university_name\"]) is not None\n",
        "    has_uid  = first_existing(df, COLUMN_MAP[\"university_id\"])   is not None\n",
        "    if not (has_name or has_uid):\n",
        "        raise ValueError(\n",
        "            \"University identifier not found. Include a name column (e.g., 'univName') or a 'university_id' column.\"\n",
        "        )\n",
        "\n",
        "def _is_numeric_scalar(v) -> bool:\n",
        "    return isinstance(v, (int, float, np.integer, np.floating)) and pd.notna(v)\n",
        "\n",
        "def make_university_display(df: pd.DataFrame, col_univ: T.Optional[str], col_uid: T.Optional[str]) -> pd.Series:\n",
        "    if col_univ is not None:\n",
        "        return df[col_univ].astype(str)\n",
        "    if col_uid is not None:\n",
        "        uid = pd.to_numeric(df[col_uid], errors=\"coerce\")\n",
        "        def _fmt(v):\n",
        "            if _is_numeric_scalar(v):\n",
        "                try:\n",
        "                    return f\"University #{int(v)}\"\n",
        "                except Exception:\n",
        "                    return f\"University {v}\"\n",
        "            return f\"University {v}\"\n",
        "        return uid.map(_fmt)\n",
        "    return pd.Series([f\"University @row[{i}]\" for i in df.index], index=df.index)\n",
        "\n",
        "def build_features(df: pd.DataFrame, cols: dict, user_inputs: dict) -> T.Tuple[pd.DataFrame, dict]:\n",
        "    feats = pd.DataFrame(index=df.index)\n",
        "    detail = {\"applied\": {}}\n",
        "    if cols.get(\"rank\") is not None:\n",
        "        rnorm = 1.0 - minmax(df[cols[\"rank\"]])\n",
        "        feats[\"rank_util\"] = rnorm\n",
        "        detail[\"applied\"][\"rank\"] = {\"kind\": \"item_only\", \"col\": cols[\"rank\"]}\n",
        "    for key in [\"gre\", \"toefl\", \"gpa\", \"sop\", \"lor\", \"research\"]:\n",
        "        user_val = user_inputs.get(key, None)\n",
        "        colname = cols.get(key, None)\n",
        "        if user_val is None:\n",
        "            continue\n",
        "        if key == \"research\":\n",
        "            if colname is not None:\n",
        "                item = pd.to_numeric(df[colname], errors=\"coerce\").fillna(0)\n",
        "                util = (item > 0).astype(float) * (1.0 if float(user_val) > 0 else 0.0)\n",
        "            else:\n",
        "                util = np.full(len(df), normalize_user_prior(key, user_val), dtype=float)\n",
        "        else:\n",
        "            if colname is not None:\n",
        "                item = pd.to_numeric(df[colname], errors=\"coerce\")\n",
        "                item_norm = minmax(item)\n",
        "                umin, umax = item.min(skipna=True), item.max(skipna=True)\n",
        "                if pd.isna(umin) or pd.isna(umax) or umax == umin:\n",
        "                    user_norm = 0.5\n",
        "                else:\n",
        "                    user_norm = float(np.clip((float(user_val) - umin) / ((umax - umin) + 1e-9), 0.0, 1.0))\n",
        "                util = 1.0 - np.abs(item_norm - user_norm)\n",
        "            else:\n",
        "                util = np.full(len(df), normalize_user_prior(key, user_val), dtype=float)\n",
        "        feats[f\"{key}_util\"] = util\n",
        "        detail[\"applied\"][key] = {\"kind\": \"match\" if colname is not None else \"prior\", \"col\": colname}\n",
        "    return feats, detail\n",
        "\n",
        "def score_items(feats: pd.DataFrame) -> T.Tuple[pd.Series, dict]:\n",
        "    util = pd.Series(np.zeros(len(feats), dtype=float), index=feats.index)\n",
        "    contribs = {}\n",
        "    for key, w in WEIGHTS.items():\n",
        "        col = \"rank_util\" if key == \"rank\" else f\"{key}_util\"\n",
        "        if col in feats:\n",
        "            term = w * feats[col].astype(float)\n",
        "            util = util + term\n",
        "            contribs[key] = term\n",
        "    p = pd.Series(sigmoid(util.values), index=feats.index, name=\"prob\")\n",
        "    return p, contribs\n",
        "\n",
        "def explain_top_contributors(contribs: dict, idx: int, top_k: int = 3) -> T.List[str]:\n",
        "    rows = []\n",
        "    for k, series in contribs.items():\n",
        "        val = float(series.loc[idx])\n",
        "        rows.append((k, val, abs(val)))\n",
        "    rows.sort(key=lambda x: x[2], reverse=True)\n",
        "    out = []\n",
        "    for k, val, _ in rows[:top_k]:\n",
        "        direction = \"increased\" if val >= 0 else \"decreased\"\n",
        "        out.append(f\"{k.upper()} {direction} the score ({val:+.3f}).\")\n",
        "    return out\n",
        "\n",
        "def load_and_prepare(df: pd.DataFrame):\n",
        "    validate_dataset(df)\n",
        "    cols = {\n",
        "        \"university_name\": first_existing(df, COLUMN_MAP[\"university_name\"]),\n",
        "        \"university_id\": first_existing(df, COLUMN_MAP[\"university_id\"]),\n",
        "        \"rank\": first_existing(df, COLUMN_MAP[\"rank\"]),\n",
        "        \"region\": first_existing(df, COLUMN_MAP[\"region\"]),\n",
        "        \"gre\": first_existing(df, COLUMN_MAP[\"gre\"]),\n",
        "        \"toefl\": first_existing(df, COLUMN_MAP[\"toefl\"]),\n",
        "        \"gpa\": first_existing(df, COLUMN_MAP[\"gpa\"]),\n",
        "        \"sop\": first_existing(df, COLUMN_MAP[\"sop\"]),\n",
        "        \"lor\": first_existing(df, COLUMN_MAP[\"lor\"]),\n",
        "        \"research\": first_existing(df, COLUMN_MAP[\"research\"]),\n",
        "        \"label\": first_existing(df, COLUMN_MAP[\"label\"]),\n",
        "    }\n",
        "    uni_display = make_university_display(df, cols[\"university_name\"], cols[\"university_id\"])\n",
        "    return df, cols, uni_display\n",
        "\n",
        "def template_1_topn(df: pd.DataFrame, gre: float, toefl: float, gpa: float, sop: float, lor: float, research: int, top_n: int = TOP_N_DEFAULT):\n",
        "    df, cols, uni_display = load_and_prepare(df)\n",
        "    user = {\"gre\": gre, \"toefl\": toefl, \"gpa\": gpa, \"sop\": sop, \"lor\": lor, \"research\": research}\n",
        "    feats, _ = build_features(df, cols, user)\n",
        "    probs, contribs = score_items(feats)\n",
        "    top_idx = probs.sort_values(ascending=False).head(int(top_n)).index\n",
        "    rows = []\n",
        "    for i, idx in enumerate(top_idx, 1):\n",
        "        drivers = \"; \".join(explain_top_contributors(contribs, idx, top_k=3))\n",
        "        rows.append({\n",
        "            \"Rank #\": i,\n",
        "            \"University\": uni_display.loc[idx],\n",
        "            \"Predicted Admit Probability\": float(probs.loc[idx]),\n",
        "            \"Drivers (Top 3)\": drivers\n",
        "        })\n",
        "    out_df = pd.DataFrame(rows)\n",
        "    names_only = \"\\n\".join(f\"- {u}\" for u in out_df[\"University\"].astype(str).tolist())\n",
        "    return out_df, names_only\n",
        "\n",
        "# ---------------------\n",
        "# Kaggle dataset loader\n",
        "# ---------------------\n",
        "KAGGLE_DATASET_SLUG = \"nitishabharathi/university-recommendation\"\n",
        "\n",
        "def _load_kaggle_df() -> tuple[pd.DataFrame, str]:\n",
        "    \"\"\"\n",
        "    Download the Kaggle dataset locally and load a CSV.\n",
        "    Returns (DataFrame, loaded_csv_path).\n",
        "    Tries to locate a CSV automatically; prefers names containing 'original' if present.\n",
        "    \"\"\"\n",
        "    # Download latest dataset locally (directory path)\n",
        "    try:\n",
        "        local_dir = kagglehub.dataset_download(KAGGLE_DATASET_SLUG)\n",
        "    except Exception:\n",
        "        # Fallback: attempt using the Pandas adapter with common file names\n",
        "        for cand in [\"Original.csv\", \"original.csv\", \"Admission_Predict.csv\", \"Admission_Predict_Ver1.1.csv\"]:\n",
        "            try:\n",
        "                df = kagglehub.load_dataset(KaggleDatasetAdapter.PANDAS, KAGGLE_DATASET_SLUG, cand)\n",
        "                return df, f\"[adapter]{cand}\"\n",
        "            except Exception:\n",
        "                pass\n",
        "        raise RuntimeError(\"Could not download or load a CSV from the Kaggle dataset.\")\n",
        "\n",
        "    # Search for CSV files inside local_dir\n",
        "    csvs = glob.glob(os.path.join(local_dir, \"**\", \"*.csv\"), recursive=True)\n",
        "    if not csvs:\n",
        "        raise RuntimeError(f\"No CSV files found in the Kaggle dataset at: {local_dir}\")\n",
        "\n",
        "    # Prefer file names containing 'original', else first CSV\n",
        "    preferred = [p for p in csvs if \"original\" in os.path.basename(p).lower()]\n",
        "    chosen = preferred[0] if preferred else csvs[0]\n",
        "\n",
        "    # Robust encoding attempts\n",
        "    last_err = None\n",
        "    for enc in (None, \"utf-8\", \"utf-8-sig\", \"latin-1\"):\n",
        "        try:\n",
        "            df = pd.read_csv(chosen, encoding=enc)\n",
        "            return df, chosen\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "    raise RuntimeError(f\"Failed to read CSV {chosen}: {type(last_err).__name__}: {last_err}\")\n",
        "\n",
        "# Cache the Kaggle DataFrame on first use\n",
        "_cached_df = None\n",
        "_cached_path = None\n",
        "\n",
        "def get_default_df() -> tuple[pd.DataFrame, str]:\n",
        "    global _cached_df, _cached_path\n",
        "    if _cached_df is None:\n",
        "        _cached_df, _cached_path = _load_kaggle_df()\n",
        "    return _cached_df, _cached_path\n",
        "\n",
        "# ---------------------\n",
        "# Gradio helpers\n",
        "# ---------------------\n",
        "def _coerce_to_df(file_obj) -> pd.DataFrame:\n",
        "    \"\"\"Accepts gr.File input and returns a DataFrame with robust encoding handling.\"\"\"\n",
        "    if file_obj is None:\n",
        "        raise ValueError(\"Please upload a CSV file or choose 'Kaggle (auto)'.\")\n",
        "    path = None\n",
        "    if isinstance(file_obj, (str, os.PathLike)):\n",
        "        path = str(file_obj)\n",
        "    elif hasattr(file_obj, \"name\"):\n",
        "        path = str(file_obj.name)\n",
        "    elif isinstance(file_obj, dict) and \"name\" in file_obj:\n",
        "        path = str(file_obj[\"name\"])\n",
        "    else:\n",
        "        # Fallback: try bytes\n",
        "        try:\n",
        "            content = file_obj.read()\n",
        "            for enc in (None, \"utf-8\", \"utf-8-sig\", \"latin-1\"):\n",
        "                try:\n",
        "                    return pd.read_csv(io.BytesIO(content), encoding=enc)\n",
        "                except Exception:\n",
        "                    pass\n",
        "            raise ValueError(\"Could not decode uploaded CSV bytes.\")\n",
        "        except Exception:\n",
        "            pass\n",
        "    if path is None or not os.path.exists(path):\n",
        "        raise FileNotFoundError(\"Could not resolve the uploaded file path.\")\n",
        "    # Try multiple encodings\n",
        "    last_err = None\n",
        "    for enc in (None, \"utf-8\", \"utf-8-sig\", \"latin-1\"):\n",
        "        try:\n",
        "            return pd.read_csv(path, encoding=enc)\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "    raise RuntimeError(f\"Failed to read CSV: {type(last_err).__name__}: {last_err}\")\n",
        "\n",
        "def ui_topn(source, file_obj, gre, toefl, gpa, sop, lor, research, top_n):\n",
        "    # Resolve dataset source\n",
        "    if source == \"Kaggle (auto)\":\n",
        "        try:\n",
        "            df_raw, path_used = get_default_df()\n",
        "            loaded_from = f\"Kaggle: {KAGGLE_DATASET_SLUG}\\nFile: {path_used}\"\n",
        "        except Exception as e:\n",
        "            msg = f\"Error loading Kaggle dataset: {type(e).__name__}: {e}\"\n",
        "            return pd.DataFrame([{\"Status\": msg}]), msg, \"\"\n",
        "    else:\n",
        "        try:\n",
        "            df_raw = _coerce_to_df(file_obj)\n",
        "            loaded_from = \"Uploaded CSV\"\n",
        "        except Exception as e:\n",
        "            msg = f\"Error reading uploaded CSV: {type(e).__name__}: {e}\"\n",
        "            return pd.DataFrame([{\"Status\": msg}]), msg, \"\"\n",
        "\n",
        "    # Run recommender\n",
        "    try:\n",
        "        out_df, names_only = template_1_topn(\n",
        "            df=df_raw,\n",
        "            gre=float(gre), toefl=float(toefl), gpa=float(gpa),\n",
        "            sop=float(sop), lor=float(lor), research=int(research),\n",
        "            top_n=int(top_n)\n",
        "        )\n",
        "        return out_df, names_only, loaded_from\n",
        "    except Exception as e:\n",
        "        msg = f\"Error during recommendation: {type(e).__name__}: {e}\"\n",
        "        return pd.DataFrame([{\"Status\": msg}]), msg, loaded_from\n",
        "\n",
        "# ---------------------\n",
        "# Gradio app (with Kaggle source option)\n",
        "# ---------------------\n",
        "with gr.Blocks(title=\"University Recommender — Top-N\") as demo:\n",
        "    gr.Markdown(\"## University Recommender — Top-N\")\n",
        "    gr.Markdown(\"Choose data source, set the applicant profile, and get Top-N universities. Defaults to **Kaggle (auto)**.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        source = gr.Radio(choices=[\"Kaggle (auto)\", \"Upload CSV\"], value=\"Kaggle (auto)\", label=\"Data Source\")\n",
        "        dataset = gr.File(label=\"Upload CSV (only used if 'Upload CSV' selected)\", file_types=[\".csv\"])\n",
        "        top_n = gr.Number(value=5, precision=0, label=\"Top-N\")\n",
        "\n",
        "    with gr.Row():\n",
        "        gre = gr.Number(value=320, label=\"GRE\")\n",
        "        toefl = gr.Number(value=105, label=\"TOEFL\")\n",
        "        gpa = gr.Number(value=3.6, label=\"GPA\")\n",
        "\n",
        "    with gr.Row():\n",
        "        sop = gr.Number(value=4.0, label=\"SOP (1–5)\")\n",
        "        lor = gr.Number(value=4.0, label=\"LOR (1–5)\")\n",
        "        research = gr.Dropdown(choices=[0, 1], value=1, label=\"Research (0/1)\")\n",
        "\n",
        "    run_btn = gr.Button(\"Recommend\")\n",
        "\n",
        "    out_table = gr.Dataframe(interactive=False, label=\"Top-N Table\")\n",
        "    out_names = gr.Textbox(label=\"University Names Only\", lines=8)\n",
        "    out_src = gr.Textbox(label=\"Loaded From\", interactive=False)\n",
        "\n",
        "    run_btn.click(\n",
        "        fn=ui_topn,\n",
        "        inputs=[source, dataset, gre, toefl, gpa, sop, lor, research, top_n],\n",
        "        outputs=[out_table, out_names, out_src]\n",
        "    )\n",
        "\n",
        "# Launch and print share URL for Colab\n",
        "launch_info = demo.launch(share=True, inbrowser=False, prevent_thread_lock=True)\n",
        "\n",
        "def _extract_share_url(obj):\n",
        "    # Compatible across Gradio versions (obj may be object or tuple/list)\n",
        "    url = getattr(obj, \"share_url\", None)\n",
        "    if url:\n",
        "        return url\n",
        "    if isinstance(obj, (list, tuple)):\n",
        "        # Newer gradio often returns (app, local_url, share_url)\n",
        "        if len(obj) >= 3 and isinstance(obj[2], str):\n",
        "            return obj[2]\n",
        "        if len(obj) >= 2 and isinstance(obj[1], str) and obj[1].startswith(\"http\"):\n",
        "            return obj[1]\n",
        "    return None\n",
        "\n",
        "share_url = _extract_share_url(launch_info)\n",
        "print(\"\\n================ Gradio URL ================\")\n",
        "print(f\" Public: {share_url or 'N/A'}\")\n",
        "try:\n",
        "    local_url = getattr(launch_info, \"local_url\", None)\n",
        "except Exception:\n",
        "    local_url = None\n",
        "print(f\"  Local:  {local_url or 'N/A'}\")\n",
        "print(\"============================================\\n\")\n"
      ]
    }
  ]
}